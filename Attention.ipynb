{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9079969-cc11-49f0-affe-a9ca5e1647ee",
   "metadata": {},
   "source": [
    "# Encoder Block \n",
    "For better understanding and visual follow this video by Umar Jamil \n",
    "Coding a Transformer from scratch on PyTorch, with full explanation, training and inference\n",
    "https://www.youtube.com/watch?v=ISNdQcPhsts&t=3118s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62a2c30c-8aef-414c-a6ea-83ef687ecf29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a77fd6-7ce5-40ab-aec1-d70977d7c151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model:int , vocab_size: int): #constructor to define the variabls\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # embedding lenght\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # length of sentence * embedding dimension  and return a \n",
    "        # a same vector of embedding which will be multiplied later to get the emedding for each word\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # in this we try to normalize the embedding as per paper\n",
    "        return self.embedding(x) *math.sqrt(self.d_model)\n",
    "    \n",
    "    \n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int , seq_len: int, dropout:float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout) # we need to create a dropput layer to reduce overfitting\n",
    "        \n",
    "        # now we need to create a positionla encoding as per transformer paper\n",
    "        # create matrix of length (Seq_len,d_model)\n",
    "        pe= torch.zeros(seq_len,d_model)\n",
    "        \n",
    "        #Create matrix of length (Seq_len,1) gives th position of each word in sentence\n",
    "        position=torch.arange(0,seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term= torch.exp(torch.arange(0,d_model,2).float() * (-math.log(1000.0)/d_model))\n",
    "        \n",
    "        #apply sin and cosine\n",
    "        pe[:,0::2]=torch.sin(position*div_term)\n",
    "        pe[:,1::2]=torch.cos(position*div_term)\n",
    "        \n",
    "        \n",
    "        pe=pe.unsqueeze(0) #(1, Seq_Len,d_model)\n",
    "        \n",
    "        self.register_buffer('pe',pe)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        # adding pe to embedding of every word and making sure that this layer is not trainable\n",
    "        x= x + (self.pe[:,:x.shape[1],:].require_grad(False))\n",
    "        \n",
    "  \n",
    "        \n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps:float = 10**-6) -> None: # the eps si avoid if during normalizatin the value in the dnmoniator ios 0\n",
    "        super().__init__()\n",
    "        self.eps =eps\n",
    "        self.aplha=nn.Parameter(torch.ones(1)) #mulitplied\n",
    "        self.bias=nn.Paramters(torch.ones(1)) #Added\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        mean=x.mean(dim=-1, keepdim=True)\n",
    "        std=x.std(dim=-1, keepdim=True)\n",
    "        return self.alpha *(x-mean)/(std+self.eps) + self.bias\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a5bc673-feba-4182-aa23-719fe0522a28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model:int, d_ff:int, dropout: float) -> None :\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model,d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff,d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c0f7213-7a9e-4887-a3d6-f195405490ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int,h:int,dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.dropout= dropout\n",
    "        \n",
    "        assert d_model %h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.w_q = nn.Linear(d_model,d_model)\n",
    "        self.w_k = nn.Linear(d_model,d_model)\n",
    "        self.w_v = nn.Linear(d_model,d_model)\n",
    "        \n",
    "        self.w_o = nn.Linear(d_model,d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(query,key,value,mask,dropout:nn.Dropout):\n",
    "        d_k=qurery.shape[-1]\n",
    "        \n",
    "        # (Batch,h, seq_len, d_k) -->(Batch,h, seq_len, seq_len)\n",
    "        attention_score= (query@key.transpose(-2,-1))/math.sqrt(d_k)\n",
    "        \n",
    "        \n",
    "        #masking\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill(mask==0,-1e9)\n",
    "        \n",
    "        attention_scores=attention_scores.softmax(dim=-1)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            attention_scores=dropout(attention_scores)\n",
    "            \n",
    "        return (attention_scores @value),attention_scores\n",
    "            \n",
    "        \n",
    "    def forward(self,q,k,v,mask):\n",
    "        query = self.w_q(q)   # (Batch, seq_len, d_model) --> (Batch,seq_len,d_model)\n",
    "        ke = self.w_k(q)    # (Batch, seq_len, d_model) --> (Batch,seq_len,d_model)\n",
    "        value = self.w_v(v)   # (Batch, seq_len, d_model) --> (Batch,seq_len,d_model)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # (Batch, seq_len, d_model) --> # (Batch, seq_len, h, d_k) --> # (Batch,h, seq_len, d_k)\n",
    "        query= query.view(query.shape[0],query,shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        key= key.view(key.shape[0],key,shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        value= value.view(value.shape[0],value,shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        \n",
    "        x,self.attention_scores=MultiHeadAttentionBlock.attention(query,key,value,mask,self.dropout)\n",
    "        \n",
    "        #(Batch,h, seq_len, d_k) -->(Batch, seq_len,h, d_k)-->(Batch,h, seq_len, d_k)\n",
    "        x=x.transpose(1,2).contiguous().view(x.shape[0],-1,self.h *self.d_k)\n",
    "         #(Batch,h, seq_len, d_model) --> #(Batch,h, seq_len, d_model)\n",
    "        return self.w_o(x)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00c9f88c-fe4a-4980-b268-22c74f092c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self,dropout:float)->None:\n",
    "        super().__init__()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "        \n",
    "    \n",
    "    def forward(self,x,sublayer):\n",
    "        return x+self.dropout(sublayer(self.norm(x)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916f9645-6101-4842-85f4-3659e3c449c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock,dropout:float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block=self_attention_block\n",
    "        self.feedforward_block= feedforward_block\n",
    "        self.residual_connection=nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "        \n",
    "    def forward(self,x ,src_mask):\n",
    "        \n",
    "        #calling self attention\n",
    "        x=self.residual_connections[0](x,lambda x: self.self_attention_block(x,x,x,src_mask))\n",
    "        \n",
    "        # feed forwar connection block\n",
    "        x=self.residual_connections[1](x,self.feed_forward_block)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8c3e15b-26b4-4265-9e8c-6576ac7b985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # module list is used to apply one after anonterh\n",
    "    def __init__(self,layers:nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers= layers\n",
    "        self.norm =LayerNormalization()\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x=layer(x,mask)\n",
    "            \n",
    "        return self.norm(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a6a0f-aacc-40f1-92ad-26e99d5fbdfe",
   "metadata": {},
   "source": [
    "# Decoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32a7f02-36ad-4d93-8f60-e72fcf49ea89",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89d26b53-7d96-442a-8e87-03149b4b6017",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,self_attention_block: MultiHeadAttentionBlock,cross_attention_block: MultiHeadAttentionBlock, feed_forward: FeedForwardBlock,drop_out=float)-> None:\n",
    "        \n",
    "        self.self_attention_block=self_attention_block\n",
    "        self.cross_attention_block=cross_attention_block\n",
    "        self.feed_forward=feed_forward\n",
    "        \n",
    "        # we ahve 3 residual connections. In decoder block we have 3  connections while in encoder we have 2 residual connections\n",
    "        self.residual_connections=nn.Module([ResidualConnection(dropout) for _ in range (3)])\n",
    "        \n",
    "    def forward(self,x ,encoder_output,src_mask,tgt_mask):\n",
    "        # this residual connection is valuse from decoder input while the next one will usques from decoder, key and value form encoder,\n",
    "        #mask from encoder\n",
    "        x=self.residual_connection[0](x,lambda x:self.self_attention_block(x,x,x,tgt_mask))\n",
    "        x=self.residual_connection[1](x,lambda x:self.self_attention_block(x,encoder_output,encoder_output,src_mask))\n",
    "        x=self.residual_connection[2](x,lambda x:self.feed_forward_block)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers: nn.Module)-> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.layers=layers\n",
    "        self.norm=LayerNormalization()\n",
    "    \n",
    "        \n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x= layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "                                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f358db46-1c65-4987-9890-22e16b223823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size:int)-> None:\n",
    "        super().__init__()\n",
    "        self.proj=nn.Linear(d_model,vocab_size)\n",
    "        \n",
    "    def forward (self,x):\n",
    "        \n",
    "        #(Batch, se_len,d_model)-->(batch,seq_len,Vocab_size)\n",
    "        \n",
    "        return torch.log_softmax(self.projec\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
