{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62a2c30c-8aef-414c-a6ea-83ef687ecf29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a77fd6-7ce5-40ab-aec1-d70977d7c151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model:int , vocab_size: int): #constructor to define the variabls\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # embedding lenght\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # length of sentence * embedding dimension  and return a \n",
    "        # a same vector of embedding which will be multiplied later to get the emedding for each word\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # in this we try to normalize the embedding\n",
    "        return self.embedding(x) *math.sqrt(self.d_model)\n",
    "    \n",
    "    \n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int , seq_len: int, dropout:float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout) # we need to create a dropput layer to reduce overfitting\n",
    "        \n",
    "        # now we need to create a positionla encoding as per transformer paper\n",
    "        # create matrix of length (Seq_len,d_model)\n",
    "        pe= torch.zeros(seq_len,d_model)\n",
    "        \n",
    "        #Create matrix of length (Seq_len,1) gives th position of each word in sentence\n",
    "        position=torch.arange(0,seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term= torch.exp(torch.arange(0,d_model,2).float() * (-math.log(1000.0)/d_model))\n",
    "        \n",
    "        #apply sin and cosine\n",
    "        pe[:,0::2]=torch.sin(position*div_term)\n",
    "        pe[:,1::2]=torch.cos(position*div_term)\n",
    "        \n",
    "        \n",
    "        pe=pe.unsqueeze(0) #(1, Seq_Len,d_model)\n",
    "        \n",
    "        self.register_buffer('pe',pe)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        # adding pe to embedding of every word and making sure that this layer is not trainable\n",
    "        x= x + (self.pe[:,:x.shape[1],:].require_grad(False))\n",
    "        \n",
    "  \n",
    "        \n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps:float = 10**-6) -> None: # the eps si avoid if during normalizatin the value in the dnmoniator ios 0\n",
    "        super().__init__()\n",
    "        self.eps =eps\n",
    "        self.aplha=nn.Parameter(torch.ones(1)) #mulitplied\n",
    "        self.bias=nn.Paramters(torch.ones(1)) #Added\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        mean=x.mean(dim=-1, keepdim=True)\n",
    "        std=x.std(dim=-1, keepdim=True)\n",
    "        return self.alpha *(x-mean)/(std+self.eps) + self.bias\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a5bc673-feba-4182-aa23-719fe0522a28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model:int, d_ff:int, dropout: float) -> None :\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model,d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff,d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c0f7213-7a9e-4887-a3d6-f195405490ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int,h:int,dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.dropout= dropout\n",
    "        \n",
    "        assert d_model %h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.w_q = nn.Linear(d_model,d_model)\n",
    "        self.w_k = nn.Linear(d_model,d_model)\n",
    "        self.w_v = nn.Linear(d_model,d_model)\n",
    "        \n",
    "        self.w_o = nn.Linear(d_model,d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(query,key,value,mask,dropout:nn.Dropout):\n",
    "        d_k=qurery.shape[-1]\n",
    "        \n",
    "        # (Batch,h, seq_len, d_k) -->(Batch,h, seq_len, seq_len)\n",
    "        attention_score= (query@key.transpose(-2,-1))/math.sqrt(d_k)\n",
    "        \n",
    "        \n",
    "        #masking\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill(mask==0,-1e9)\n",
    "        \n",
    "        attention_scores=attention_scores.softmax(dim=-1)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            attention_scores=dropout(attention_scores)\n",
    "            \n",
    "        return (attention_scores @value),attention_scores\n",
    "            \n",
    "        \n",
    "    def forward(self,q,k,v,mask):\n",
    "        query = self.w_q(q)   # (Batch, seq_len, d_model) --> (Batch,seq_len,d_model)\n",
    "        ke = self.w_k(q)    # (Batch, seq_len, d_model) --> (Batch,seq_len,d_model)\n",
    "        value = self.w_v(v)   # (Batch, seq_len, d_model) --> (Batch,seq_len,d_model)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # (Batch, seq_len, d_model) --> # (Batch, seq_len, h, d_k) --> # (Batch,h, seq_len, d_k)\n",
    "        query= query.view(query.shape[0],query,shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        key= key.view(key.shape[0],key,shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        value= value.view(value.shape[0],value,shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        \n",
    "        x,self.attention_scores=MultiHeadAttentionBlock.attention(query,key,value,mask,self.dropout)\n",
    "        \n",
    "        #(Batch,h, seq_len, d_k) -->(Batch, seq_len,h, d_k)-->(Batch,h, seq_len, d_k)\n",
    "        x=x.transpose(1,2).contiguous().view(x.shape[0],-1,self.h *self.d_k)\n",
    "         #(Batch,h, seq_len, d_model) --> #(Batch,h, seq_len, d_model)\n",
    "        return self.w_o(x)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00c9f88c-fe4a-4980-b268-22c74f092c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self,dropout:float)->None:\n",
    "        super().__init__()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "        \n",
    "    \n",
    "    def forward(self,x,sublayer):\n",
    "        return x+self.dropout(sublayer(self.norm(x)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "916f9645-6101-4842-85f4-3659e3c449c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock,dropout:float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block=self_attention_block\n",
    "        self.feedforward_block= feedforward_block\n",
    "        self.residual_connection=nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "        \n",
    "    def forward(self,x ,src_mask):\n",
    "        \n",
    "        x=self.residual_connections[0](x,lambda x: self.self_attention_block(x,x,x,src_mask))\n",
    "        x=self.residual_connections[1](x,self.feed_forward_block)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8c3e15b-26b4-4265-9e8c-6576ac7b985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,layers:nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers= layers\n",
    "        self.norm =LayerNormalization()\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x=layer(x,mask)\n",
    "            \n",
    "        return self.norm(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9605b8a-576d-4742-8d22-ceffba1e3fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tensorflow in /home/naseem_fordham/.local/lib/python3.8/site-packages (2.13.1)\n",
      "Collecting keras\n",
      "  Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.14,>=2.13.0 in /home/naseem_fordham/.local/lib/python3.8/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<2.14,>=2.13 in /home/naseem_fordham/.local/lib/python3.8/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: flatbuffers>=23.1.21 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.3.3)\n",
      "Requirement already satisfied, skipping upgrade: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: astunparse>=1.6.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.6.2)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.1 in /usr/lib/python3/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
      "Requirement already satisfied, skipping upgrade: gast<=0.4.0,>=0.2.1 in /usr/lib/python3/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\" in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /home/naseem_fordham/.local/lib/python3.8/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied, skipping upgrade: grpcio<2.0,>=1.24.3 in /home/naseem_fordham/.local/lib/python3.8/site-packages (from tensorflow) (1.48.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Using cached protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/lib/python3/dist-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /home/naseem_fordham/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.6)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.8.0,>=0.7.0 in /home/naseem_fordham/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.2)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.16.2)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<1.1,>=0.5 in /home/naseem_fordham/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<3,>=1.21.1 in /home/naseem_fordham/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (1.26.18)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.12)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.2)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<6.0,>=2.0.0 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.0.0)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/lib/python3/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "\u001b[31mERROR: tensorflow 2.13.1 has requirement keras<2.14,>=2.13.1, but you'll have keras 2.15.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: wandb 0.12.21 has requirement protobuf<4.0dev,>=3.12.0, but you'll have protobuf 4.25.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorboardx 2.6 has requirement protobuf<4,>=3.8.0, but you'll have protobuf 4.25.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: keras, protobuf\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.13.1\n",
      "    Uninstalling keras-2.13.1:\n",
      "      Successfully uninstalled keras-2.13.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.12.0\n",
      "    Uninstalling protobuf-3.12.0:\n",
      "      Successfully uninstalled protobuf-3.12.0\n",
      "Successfully installed keras-2.15.0 protobuf-4.25.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c19cdeb2-fda2-4d83-abb2-f5e5c82a2e84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.13.1\n",
      "  Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: grpcio==1.48.2 in /home/naseem_fordham/.local/lib/python3.8/site-packages (1.48.2)\n",
      "Collecting protobuf==3.12.0\n",
      "  Using cached protobuf-3.12.0-cp38-cp38-manylinux1_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: wandb==0.12.21 in /usr/local/lib/python3.8/dist-packages (0.12.21)\n",
      "Requirement already satisfied: tensorboardx==2.6 in /usr/local/lib/python3.8/dist-packages (2.6)\n",
      "Requirement already satisfied: six>=1.5.2 in /usr/lib/python3/dist-packages (from grpcio==1.48.2) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from protobuf==3.12.0) (45.2.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.12.21) (1.0.11)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.12.21) (1.16.0)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb==0.12.21) (0.1.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.12.21) (5.9.4)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.12.21) (8.1.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.12.21) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from wandb==0.12.21) (5.3.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/naseem_fordham/.local/lib/python3.8/site-packages (from wandb==0.12.21) (2.31.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.12.21) (2.3)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.12.21) (3.1.31)\n",
      "Requirement already satisfied: setproctitle in /usr/lib/python3/dist-packages (from wandb==0.12.21) (1.1.10)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardx==2.6) (1.24.2)\n",
      "Requirement already satisfied: packaging in /home/naseem_fordham/.local/lib/python3.8/site-packages (from tensorboardx==2.6) (23.2)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from sentry-sdk>=1.0.0->wandb==0.12.21) (2019.11.28)\n",
      "Requirement already satisfied: urllib3>=1.26.11; python_version >= \"3.6\" in /home/naseem_fordham/.local/lib/python3.8/site-packages (from sentry-sdk>=1.0.0->wandb==0.12.21) (1.26.18)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb==0.12.21) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb==0.12.21) (2.0.12)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb==0.12.21) (4.0.10)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb==0.12.21) (5.0.0)\n",
      "\u001b[31mERROR: tensorflow 2.13.1 has requirement protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you'll have protobuf 3.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorboard 2.13.0 has requirement protobuf>=3.19.6, but you'll have protobuf 3.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-api-core 2.14.0 has requirement protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you'll have protobuf 3.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: googleapis-common-protos 1.58.0 has requirement protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you'll have protobuf 3.12.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: keras, protobuf\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.15.0\n",
      "    Uninstalling keras-2.15.0:\n",
      "      Successfully uninstalled keras-2.15.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.0\n",
      "    Uninstalling protobuf-4.25.0:\n",
      "      Successfully uninstalled protobuf-4.25.0\n",
      "Successfully installed keras-2.13.1 protobuf-3.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.13.1 grpcio==1.48.2 protobuf==3.12.0 wandb==0.12.21 tensorboardx==2.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd4ef71c-cc2b-4c28-b0d2-b558c1e8fda6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==3.20.3\n",
      "  Using cached protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.12.0\n",
      "    Uninstalling protobuf-3.12.0:\n",
      "      Successfully uninstalled protobuf-3.12.0\n",
      "Successfully installed protobuf-3.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade protobuf==3.20.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8083936-f1ce-45f1-90ad-6982c34f01f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naseem_fordham/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.13.1\n",
      "Keras version: 2.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffe4f11d-2245-4fb3-b110-8e8eead2c52e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/naseem_fordham/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/naseem_fordham/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\",20)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "from transformers import TFAutoModel, BertTokenizerFast, BertModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks as cb\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Concatenate, Flatten, Embedding, Dense, Dropout, LSTM\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2138f0-26d2-44f7-940a-a2a8998d194c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
